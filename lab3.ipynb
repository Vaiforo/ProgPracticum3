{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ES_HyperNEAT'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Импорт из ES-HyperNEAT (предполагается, что установлен)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mES_HyperNEAT\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhyperneat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HyperNEAT\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mES_HyperNEAT\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msubstrate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Substrate\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Функция генерации датасета (как раньше)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ES_HyperNEAT'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Импорт из ES-HyperNEAT (предполагается, что установлен)\n",
    "from ES_HyperNEAT.hyperneat import HyperNEAT\n",
    "from ES_HyperNEAT.substrate import Substrate\n",
    "\n",
    "# Функция генерации датасета (как раньше)\n",
    "\n",
    "\n",
    "def generate_feature(feature_type, length):\n",
    "    if feature_type == \"binary\":\n",
    "        return np.random.choice([0, 1], size=length)\n",
    "    elif feature_type == \"nominal\":\n",
    "        return np.random.choice(['A', 'B', 'C', 'D'], size=length)\n",
    "    elif feature_type == \"ordinal\":\n",
    "        return np.random.choice([1, 2, 3, 4, 5], size=length)\n",
    "    elif feature_type == \"quantitative\":\n",
    "        return np.random.uniform(0, 100, size=length)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown feature type\")\n",
    "\n",
    "\n",
    "def create_dataset(num_features, num_samples):\n",
    "    feature_types = [\"binary\", \"nominal\", \"ordinal\", \"quantitative\"]\n",
    "    chosen_types = random.sample(feature_types, k=min(4, num_features))\n",
    "    while len(chosen_types) < num_features:\n",
    "        chosen_types.append(random.choice(feature_types))\n",
    "    random.shuffle(chosen_types)\n",
    "\n",
    "    data = {}\n",
    "    for i in range(num_features):\n",
    "        data[f\"Obj1_Feat{i+1}\"] = generate_feature(\n",
    "            chosen_types[i], num_samples)\n",
    "    for i in range(num_features):\n",
    "        data[f\"Obj2_Feat{i+1}\"] = generate_feature(\n",
    "            chosen_types[i], num_samples)\n",
    "\n",
    "    labels = []\n",
    "    for idx in range(num_samples):\n",
    "        matches = sum(\n",
    "            data[f\"Obj1_Feat{f}\"][idx] == data[f\"Obj2_Feat{f}\"][idx]\n",
    "            for f in range(1, num_features+1)\n",
    "        )\n",
    "        labels.append(1 if matches >= num_features // 2 else 0)  # 1=Yes, 0=No\n",
    "    return pd.DataFrame(data), np.array(labels)\n",
    "\n",
    "# Настройка substrate — пример для ES-HyperNEAT\n",
    "\n",
    "\n",
    "def create_substrate(input_dims, output_dims=1):\n",
    "    # Входной слой — например, 2D решётка из признаков (зависит от задачи)\n",
    "    # Для простоты используем одномерную проекцию: (input_dims, 1, 1)\n",
    "    input_coordinates = [(i, 0, 0) for i in range(input_dims)]\n",
    "\n",
    "    # Выходной слой — 1 нейрон\n",
    "    output_coordinates = [(0, 0, 0)]\n",
    "\n",
    "    return Substrate(input_coordinates, output_coordinates)\n",
    "\n",
    "# Функция для оценки модели (F1)\n",
    "\n",
    "\n",
    "def evaluate_hyperneat_model(hyperneat, X_val, y_val):\n",
    "    preds = []\n",
    "    for x in X_val:\n",
    "        output = hyperneat.feed_forward(x)\n",
    "        preds.append(1 if output[0] > 0.5 else 0)\n",
    "    return f1_score(y_val, preds)\n",
    "\n",
    "# Основная функция запуска\n",
    "\n",
    "\n",
    "def run_es_hyperneat(X_train, y_train, X_val, y_val, input_dim):\n",
    "    # Создаём substrate с размером входа input_dim\n",
    "    substrate = create_substrate(input_dim)\n",
    "\n",
    "    # Настройки HyperNEAT (здесь базовые, можно тонко настраивать)\n",
    "    hyperneat = HyperNEAT(\n",
    "        substrate=substrate,\n",
    "        population_size=50,\n",
    "        max_generations=50,\n",
    "        verbosity=2,\n",
    "    )\n",
    "\n",
    "    # Запуск эволюции\n",
    "    winner = hyperneat.run(X_train, y_train)\n",
    "\n",
    "    # Оценка модели\n",
    "    f1 = evaluate_hyperneat_model(hyperneat, X_val, y_val)\n",
    "    print(f\"F1-score модели ES-HyperNEAT: {f1:.4f}\")\n",
    "\n",
    "    return winner, hyperneat\n",
    "\n",
    "\n",
    "# Путь для сохранения моделей\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Генерируем данные для малого датасета\n",
    "df_small, labels_small = create_dataset(num_features=4, num_samples=30)\n",
    "X_small = df_small.values\n",
    "y_small = labels_small\n",
    "X_train_s, X_val_s, y_train_s, y_val_s = train_test_split(\n",
    "    X_small, y_small, test_size=0.2, random_state=42)\n",
    "\n",
    "# Генерируем данные для большого датасета\n",
    "df_big, labels_big = create_dataset(num_features=15, num_samples=1500)\n",
    "X_big = df_big.values\n",
    "y_big = labels_big\n",
    "X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(\n",
    "    X_big, y_big, test_size=0.2, random_state=42)\n",
    "\n",
    "# Запускаем ES-HyperNEAT на малом датасете\n",
    "winner_s, model_s = run_es_hyperneat(\n",
    "    X_train_s, y_train_s, X_val_s, y_val_s, input_dim=X_train_s.shape[1])\n",
    "pickle.dump((winner_s, model_s), open(\n",
    "    f\"{save_dir}/es_hyperneat_small.pkl\", \"wb\"))\n",
    "\n",
    "# Запускаем ES-HyperNEAT на большом датасете\n",
    "winner_b, model_b = run_es_hyperneat(\n",
    "    X_train_b, y_train_b, X_val_b, y_val_b, input_dim=X_train_b.shape[1])\n",
    "pickle.dump((winner_b, model_b), open(\n",
    "    f\"{save_dir}/es_hyperneat_big.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер данных: 8 признаков\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "No such config file: p:\\Python\\ProgPracticum3\\config_small",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    143\u001b[39m config_big_path = \u001b[33m\"\u001b[39m\u001b[33mconfig_big\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Для 30 входных признаков\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Запуск NEAT для малого датасета\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m winner_small, config_small = \u001b[43mrun_neat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_small_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNEAT для малого датасета завершён.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Запуск NEAT для большого датасета\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mrun_neat\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, config_file, generations)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mРазмер данных: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m признаков\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Загружаем конфигурацию NEAT из файла\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m config = \u001b[43mneat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDefaultGenome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDefaultReproduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDefaultSpeciesSet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDefaultStagnation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_file\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Создаем популяцию\u001b[39;00m\n\u001b[32m    101\u001b[39m population = neat.Population(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mp:\\Python\\ProgPracticum3\\.venv\\Lib\\site-packages\\neat\\config.py:153\u001b[39m, in \u001b[36mConfig.__init__\u001b[39m\u001b[34m(self, genome_type, reproduction_type, species_set_type, stagnation_type, filename)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28mself\u001b[39m.stagnation_type = stagnation_type\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(filename):\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mNo such config file: \u001b[39m\u001b[33m'\u001b[39m + os.path.abspath(filename))\n\u001b[32m    155\u001b[39m parameters = ConfigParser()\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mException\u001b[39m: No such config file: p:\\Python\\ProgPracticum3\\config_small"
     ]
    }
   ],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import neat  # Библиотека NEAT (нейроэволюция)\n",
    "import numpy as np  # Для работы с массивами\n",
    "import pandas as pd  # Для работы с таблицами\n",
    "import random  # Для генерации случайных чисел\n",
    "import pickle  # Для сохранения моделей\n",
    "# Кодирование категориальных признаков\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score  # Метрика оценки\n",
    "from sklearn.model_selection import train_test_split  # Разделение данных\n",
    "from pathlib import Path  # Работа с путями файловой системы\n",
    "\n",
    "\n",
    "# Функция генерации признаков (дублируется для полноты)\n",
    "def generate_feature(feature_type: str, length: int):\n",
    "    \"\"\"Генерирует массив значений признака заданного типа\"\"\"\n",
    "    if feature_type == \"binary\":\n",
    "        return np.random.choice([0, 1], size=length)  # Бинарные значения\n",
    "    elif feature_type == \"nominal\":\n",
    "        return np.random.choice(['A', 'B', 'C', 'D'], size=length)  # Категории\n",
    "    elif feature_type == \"ordinal\":\n",
    "        return np.random.choice([1, 2, 3, 4, 5], size=length)  # Порядковые\n",
    "    elif feature_type == \"quantitative\":\n",
    "        return np.random.uniform(0, 100, size=length)  # Числовые\n",
    "    else:\n",
    "        raise ValueError(\"Unknown feature type\")  # Ошибка при неизвестном типе\n",
    "\n",
    "\n",
    "# Функция создания датасета (дублируется для полноты)\n",
    "def create_dataset(num_features: int, num_samples: int) -> pd.DataFrame:\n",
    "    \"\"\"Создает датасет с заданным числом признаков и образцов\"\"\"\n",
    "    feature_types = [\"binary\", \"nominal\", \"ordinal\", \"quantitative\"]\n",
    "    chosen_types = random.sample(feature_types, k=min(4, num_features))\n",
    "    while len(chosen_types) < num_features:\n",
    "        chosen_types.append(random.choice(feature_types))\n",
    "    random.shuffle(chosen_types)\n",
    "\n",
    "    data = {}\n",
    "    # Генерация признаков для первого объекта\n",
    "    for i in range(num_features):\n",
    "        data[f\"Obj1_Feat{i + 1}\"] = generate_feature(\n",
    "            chosen_types[i], num_samples)\n",
    "    # Генерация признаков для второго объекта\n",
    "    for i in range(num_features):\n",
    "        data[f\"Obj2_Feat{i + 1}\"] = generate_feature(\n",
    "            chosen_types[i], num_samples)\n",
    "\n",
    "    # Создание меток (Yes/No) на основе совпадений признаков\n",
    "    labels = []\n",
    "    for idx in range(num_samples):\n",
    "        matches = sum(\n",
    "            data[f\"Obj1_Feat{f}\"][idx] == data[f\"Obj2_Feat{f}\"][idx]\n",
    "            for f in range(1, num_features + 1)\n",
    "        )\n",
    "        labels.append(\"Yes\" if matches >= num_features // 2 else \"No\")\n",
    "    data[\"Collision\"] = labels\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Функция подготовки данных для обучения\n",
    "def prepare_dataset(num_features, num_samples):\n",
    "    \"\"\"Подготавливает данные для нейросети\"\"\"\n",
    "    df = create_dataset(num_features, num_samples)  # Создаем датасет\n",
    "    X = df.drop(columns=[\"Collision\"])  # Признаки\n",
    "    y = df[\"Collision\"].map({\"Yes\": 1, \"No\": 0})  # Преобразуем метки в числа\n",
    "\n",
    "    # Кодируем категориальные признаки\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == object:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "    return X.values, y.values  # Возвращаем numpy массивы\n",
    "\n",
    "\n",
    "# Функция оценки генома (индивидуального решения)\n",
    "def eval_genome(genome, config, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Оценивает качество генома на валидационных данных\"\"\"\n",
    "    net = neat.nn.FeedForwardNetwork.create(genome, config)  # Создаем сеть\n",
    "    predictions = []\n",
    "    for xi in X_val:\n",
    "        output = net.activate(xi)  # Получаем предсказание\n",
    "        # Бинарная классификация\n",
    "        predictions.append(1 if output[0] > 0.5 else 0)\n",
    "    return f1_score(y_val, predictions)  # Возвращаем F1-score\n",
    "\n",
    "\n",
    "# Основная функция запуска NEAT\n",
    "def run_neat(X_train, y_train, X_val, y_val, config_file, generations=50):\n",
    "    \"\"\"Запускает алгоритм NEAT для обучения\"\"\"\n",
    "    print(f\"Размер данных: {X_train.shape[1]} признаков\")\n",
    "\n",
    "    # Загружаем конфигурацию NEAT из файла\n",
    "    config = neat.Config(\n",
    "        neat.DefaultGenome,\n",
    "        neat.DefaultReproduction,\n",
    "        neat.DefaultSpeciesSet,\n",
    "        neat.DefaultStagnation,\n",
    "        config_file\n",
    "    )\n",
    "\n",
    "    # Создаем популяцию\n",
    "    population = neat.Population(config)\n",
    "\n",
    "    # Добавляем отчеты для вывода прогресса\n",
    "    population.add_reporter(neat.StdOutReporter(True))\n",
    "    population.add_reporter(neat.StatisticsReporter())\n",
    "\n",
    "    # Функция оценки всех геномов в популяции\n",
    "    def eval_genomes(genomes, config):\n",
    "        for genome_id, genome in genomes:\n",
    "            genome.fitness = eval_genome(\n",
    "                genome, config, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Запускаем эволюцию\n",
    "    winner = population.run(eval_genomes, generations)\n",
    "    return winner, config\n",
    "\n",
    "\n",
    "# Функция сохранения обученной модели\n",
    "def save_neat_model(winner, config, filename):\n",
    "    \"\"\"Сохраняет лучший геном и конфигурацию в файл\"\"\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump((winner, config), f)\n",
    "\n",
    "\n",
    "# Основной блок выполнения\n",
    "if __name__ == \"__main__\":\n",
    "    # Подготовка малого датасета (4 признака на объект, всего 8)\n",
    "    X_small, y_small = prepare_dataset(num_features=4, num_samples=30)\n",
    "    X_small = np.hstack([X_small[:, :4], X_small[:, 4:8]]\n",
    "                        )  # Объединяем признаки\n",
    "\n",
    "    # Подготовка большого датасета (15 признаков на объект, всего 30)\n",
    "    X_big, y_big = prepare_dataset(num_features=15, num_samples=1500)\n",
    "\n",
    "    # Разделение на обучающую и валидационную выборки\n",
    "    X_train_s, X_val_s, y_train_s, y_val_s = train_test_split(\n",
    "        X_small, y_small, test_size=0.2, random_state=42)\n",
    "    X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(\n",
    "        X_big, y_big, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Пути к файлам конфигурации NEAT\n",
    "    config_small_path = \"config_small\"  # Для 8 входных признаков\n",
    "    config_big_path = \"config_big\"  # Для 30 входных признаков\n",
    "\n",
    "    # Запуск NEAT для малого датасета\n",
    "    winner_small, config_small = run_neat(\n",
    "        X_train_s, y_train_s, X_val_s, y_val_s, config_small_path)\n",
    "    print(\"NEAT для малого датасета завершён.\")\n",
    "\n",
    "    # Запуск NEAT для большого датасета\n",
    "    winner_big, config_big = run_neat(\n",
    "        X_train_b, y_train_b, X_val_b, y_val_b, config_big_path)\n",
    "    print(\"NEAT для большого датасета завершён.\")\n",
    "\n",
    "    # Сохранение обученных моделей\n",
    "    save_dir = Path(\"saved_models\")\n",
    "    save_dir.mkdir(exist_ok=True)  # Создаем директорию, если не существует\n",
    "\n",
    "    # Сохраняем модель для малого датасета\n",
    "    save_neat_model(winner_small, config_small, save_dir / \"neat_small.pkl\")\n",
    "    # Сохраняем модель для большого датасета\n",
    "    save_neat_model(winner_big, config_big, save_dir / \"neat_big.pkl\")\n",
    "\n",
    "    print(\"Модели NEAT сохранены.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import numpy as np  # Для работы с массивами и математическими операциями\n",
    "import pandas as pd  # Для работы с табличными данными\n",
    "import random  # Для генерации случайных чисел\n",
    "from pathlib import Path  # Для удобной работы с путями файловой системы\n",
    "\n",
    "# Импорт функций из scikit-learn\n",
    "from sklearn.model_selection import train_test_split  # Для разделения данных\n",
    "# Для кодирования и масштабирования\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# Модель логистической регрессии\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier  # Модель дерева решений\n",
    "from sklearn.metrics import f1_score  # Метрика оценки качества\n",
    "\n",
    "# Импорт библиотеки для оптимизации роевым методом (PSO)\n",
    "import pyswarms as ps  # Алгоритм оптимизации Particle Swarm Optimization\n",
    "import pickle  # Для сохранения и загрузки моделей\n",
    "\n",
    "\n",
    "# Функция генерации признаков заданного типа\n",
    "def generate_feature(feature_type: str, length: int) -> np.ndarray:\n",
    "    \"\"\"Генерирует массив значений признака заданного типа\"\"\"\n",
    "    if feature_type == \"binary\":\n",
    "        return np.random.choice([0, 1], size=length)  # Бинарные значения (0/1)\n",
    "    elif feature_type == \"nominal\":\n",
    "        # Категориальные\n",
    "        return np.random.choice(['A', 'B', 'C', 'D'], size=length)\n",
    "    elif feature_type == \"ordinal\":\n",
    "        return np.random.choice([1, 2, 3, 4, 5], size=length)  # Порядковые\n",
    "    elif feature_type == \"quantitative\":\n",
    "        return np.random.uniform(0, 100, size=length)  # Количественные (0-100)\n",
    "    else:\n",
    "        # Ошибка при неверном типе\n",
    "        raise ValueError(\"Неизвестный тип признака\")\n",
    "\n",
    "\n",
    "# Функция создания датасета\n",
    "def create_dataset(num_features: int, num_samples: int) -> pd.DataFrame:\n",
    "    \"\"\"Создает датасет с заданным числом признаков и образцов\"\"\"\n",
    "    feature_types = [\"binary\", \"nominal\",\n",
    "                     \"ordinal\", \"quantitative\"]  # Доступные типы\n",
    "    chosen_types = random.sample(feature_types, k=min(\n",
    "        4, num_features))  # Выбираем случайные типы\n",
    "\n",
    "    # Дополняем список типов, если нужно больше признаков\n",
    "    while len(chosen_types) < num_features:\n",
    "        chosen_types.append(random.choice(feature_types))\n",
    "    random.shuffle(chosen_types)  # Перемешиваем типы\n",
    "\n",
    "    data = {}  # Словарь для хранения данных\n",
    "    # Генерируем признаки для первого объекта\n",
    "    for i in range(num_features):\n",
    "        data[f\"Obj1_Feat{i + 1}\"] = generate_feature(\n",
    "            chosen_types[i], num_samples)\n",
    "    # Генерируем признаки для второго объекта\n",
    "    for i in range(num_features):\n",
    "        data[f\"Obj2_Feat{i + 1}\"] = generate_feature(\n",
    "            chosen_types[i], num_samples)\n",
    "\n",
    "    # Создаем метки (Yes если >= половины признаков совпадают, иначе No)\n",
    "    labels = []\n",
    "    for idx in range(num_samples):\n",
    "        matches = sum(\n",
    "            data[f\"Obj1_Feat{f}\"][idx] == data[f\"Obj2_Feat{f}\"][idx]\n",
    "            for f in range(1, num_features + 1)\n",
    "        )\n",
    "        labels.append(\"Yes\" if matches >= num_features // 2 else \"No\")\n",
    "    data[\"Collision\"] = labels  # Добавляем метки в датасет\n",
    "    return pd.DataFrame(data)  # Возвращаем DataFrame\n",
    "\n",
    "\n",
    "# Функция оценки качества для PSO\n",
    "def pso_fitness(params, model_class, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Вычисляет качество модели для оптимизации PSO\"\"\"\n",
    "    n_particles = params.shape[0]  # Число частиц в рое\n",
    "    fitness = np.zeros(n_particles)  # Массив для хранения оценок\n",
    "\n",
    "    for i in range(n_particles):\n",
    "        p = params[i]  # Параметры текущей частицы\n",
    "        if model_class == LogisticRegression:\n",
    "            C = p[0]  # Параметр регуляризации\n",
    "            max_iter = int(p[1])  # Максимальное число итераций\n",
    "            max_iter = max(max_iter, 1000)  # Гарантируем минимум 1000 итераций\n",
    "            model = LogisticRegression(C=C, max_iter=max_iter, solver='lbfgs')\n",
    "        elif model_class == DecisionTreeClassifier:\n",
    "            max_depth = int(p[0])  # Максимальная глубина дерева\n",
    "            # Минимальное число образцов для разделения\n",
    "            min_samples_split = int(p[1])\n",
    "            model = DecisionTreeClassifier(\n",
    "                max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "        else:\n",
    "            # Если модель неизвестна, возвращаем худший результат\n",
    "            fitness[i] = 1.0\n",
    "            continue\n",
    "\n",
    "        # Проверка, что есть оба класса\n",
    "        if len(np.unique(y_train)) < 2:\n",
    "            fitness[i] = 1.0\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            model.fit(X_train, y_train)  # Обучаем модель\n",
    "            preds = model.predict(X_val)  # Прогнозируем на валидации\n",
    "            score = f1_score(y_val, preds)  # Вычисляем F1-score\n",
    "            fitness[i] = -score  # Минимизируем -F1 (PSO минимизирует функцию)\n",
    "        except Exception:\n",
    "            fitness[i] = 1.0  # При ошибке возвращаем худший результат\n",
    "\n",
    "    return fitness  # Возвращаем массив оценок\n",
    "\n",
    "\n",
    "# Функция оптимизации гиперпараметров через PSO\n",
    "def optimize_pso(model_class, X_train, y_train, X_val, y_val, bounds, options, iters=50):\n",
    "    \"\"\"Оптимизирует гиперпараметры модели с помощью PSO\"\"\"\n",
    "    # Инициализируем оптимизатор PSO\n",
    "    optimizer = ps.single.GlobalBestPSO(\n",
    "        n_particles=20,  # Число частиц в рое\n",
    "        dimensions=len(bounds[0]),  # Размерность пространства параметров\n",
    "        options=options,  # Параметры алгоритма PSO\n",
    "        bounds=bounds  # Границы пространства поиска\n",
    "    )\n",
    "\n",
    "    # Запускаем оптимизацию\n",
    "    best_cost, best_pos = optimizer.optimize(\n",
    "        pso_fitness, iters,  # Функция оценки и число итераций\n",
    "        model_class=model_class, X_train=X_train, y_train=y_train,\n",
    "        X_val=X_val, y_val=y_val  # Дополнительные аргументы для pso_fitness\n",
    "    )\n",
    "\n",
    "    print(f\"Лучшие параметры (PSO): {best_pos}, F1: {-best_cost:.4f}\")\n",
    "    return best_pos, -best_cost  # Возвращаем лучшие параметры и F1-score\n",
    "\n",
    "\n",
    "# Основной блок выполнения\n",
    "if __name__ == \"__main__\":\n",
    "    # Параметры генерации данных\n",
    "    num_features = 6  # Число признаков у каждого объекта\n",
    "    num_samples = 200  # Число образцов в датасете\n",
    "\n",
    "    # Генерируем датасет\n",
    "    data = create_dataset(num_features, num_samples)\n",
    "    print(data.head())  # Выводим первые строки датасета\n",
    "\n",
    "    # Разделяем на признаки (X) и целевую переменную (y)\n",
    "    X = data.drop(columns=[\"Collision\"])\n",
    "    y = data[\"Collision\"].map({\"Yes\": 1, \"No\": 0})  # Преобразуем метки в числа\n",
    "\n",
    "    # Кодируем категориальные признаки\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == object:  # Если признак категориальный\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])  # Кодируем числами\n",
    "\n",
    "    # Масштабируем количественные признаки\n",
    "    quant_cols = [col for col in X.columns if X[col].dtype in [\n",
    "        np.float64, np.float32]]\n",
    "    scaler = StandardScaler()  # Инициализируем стандартизатор\n",
    "    if quant_cols:\n",
    "        X[quant_cols] = scaler.fit_transform(X[quant_cols])  # Масштабируем\n",
    "\n",
    "    # Разбиваем данные на обучающую, валидационную и тестовую выборки\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Выводим распределение классов\n",
    "    print(\"Распределение классов в полном тренировочном наборе:\",\n",
    "          np.unique(y_train_full, return_counts=True))\n",
    "    print(\"Распределение классов в тренировочном наборе:\",\n",
    "          np.unique(y_train, return_counts=True))\n",
    "\n",
    "    # Задаем диапазоны для оптимизации гиперпараметров\n",
    "    logreg_bounds = (np.array([0.001, 100]), np.array(\n",
    "        [10.0, 1500]))  # Для логистической регрессии\n",
    "    dtree_bounds = (np.array([1, 2]), np.array([20, 20]))  # Для дерева решений\n",
    "\n",
    "    options = {'c1': 0.5, 'c2': 0.3, 'w': 0.9}  # Параметры PSO\n",
    "\n",
    "    # Оптимизируем логистическую регрессию\n",
    "    best_logreg_params, best_logreg_score = optimize_pso(\n",
    "        LogisticRegression, X_train, y_train, X_val, y_val,\n",
    "        bounds=logreg_bounds, options=options, iters=50\n",
    "    )\n",
    "\n",
    "    # Оптимизируем дерево решений\n",
    "    best_dtree_params, best_dtree_score = optimize_pso(\n",
    "        DecisionTreeClassifier, X_train, y_train, X_val, y_val,\n",
    "        bounds=dtree_bounds, options=options, iters=50\n",
    "    )\n",
    "\n",
    "    # Проверяем распределение классов перед финальным обучением\n",
    "    print(\"Распределение классов перед финальным обучением:\",\n",
    "          np.unique(y_train_full, return_counts=True))\n",
    "    if len(np.unique(y_train_full)) < 2:\n",
    "        raise ValueError(\n",
    "            \"Ошибка: в полном тренировочном наборе только один класс!\")\n",
    "\n",
    "    # Обучаем финальные модели на всех обучающих данных\n",
    "    final_logreg = LogisticRegression(\n",
    "        C=best_logreg_params[0],  # Оптимальный C\n",
    "        # Оптимальное число итераций\n",
    "        max_iter=int(max(best_logreg_params[1], 1000)),\n",
    "        solver='lbfgs'  # Алгоритм оптимизации\n",
    "    )\n",
    "    final_logreg.fit(X_train_full, y_train_full)  # Обучаем на всех данных\n",
    "\n",
    "    final_dtree = DecisionTreeClassifier(\n",
    "        max_depth=int(best_dtree_params[0]),  # Оптимальная глубина\n",
    "        # Оптимальный параметр разделения\n",
    "        min_samples_split=int(best_dtree_params[1])\n",
    "    )\n",
    "    final_dtree.fit(X_train_full, y_train_full)  # Обучаем на всех данных\n",
    "\n",
    "    # Сохраняем обученные модели\n",
    "    save_dir = Path(\"saved_models\")  # Путь к директории\n",
    "    save_dir.mkdir(exist_ok=True)  # Создаем директорию, если не существует\n",
    "\n",
    "    # Сохраняем модель логистической регрессии\n",
    "    with open(save_dir / \"best_logreg_pso_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(final_logreg, f)\n",
    "\n",
    "    # Сохраняем модель дерева решений\n",
    "    with open(save_dir / \"best_dtree_pso_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(final_dtree, f)\n",
    "\n",
    "    print(\"Модели успешно сохранены в 'saved_models'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
